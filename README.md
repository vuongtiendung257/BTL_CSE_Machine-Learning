# BTL_CSE_Machine-Learning
Dá»± Ã¡n: Dá»± Ä‘oÃ¡n cháº¥t lÆ°á»£ng rÆ°á»£u vang (Wine Quality Prediction)

Dá»± Ã¡n thuá»™c mÃ´n Há»c MÃ¡y cá»§a nhÃ³m 14, sá»­ dá»¥ng bá»™ dá»¯ liá»‡u Wine Quality â€“ White Wine (UCI Machine Learning Repository) vá»›i 4898 máº«u vÃ  11 Ä‘áº·c trÆ°ng hÃ³a há»c Ä‘á»ƒ dá»± Ä‘oÃ¡n cháº¥t lÆ°á»£ng rÆ°á»£u vang.

ğŸ¯ Má»¥c tiÃªu

PhÃ¢n loáº¡i cháº¥t lÆ°á»£ng rÆ°á»£u vang tráº¯ng thÃ nh 3 má»©c:

Tháº¥p (quality < 5)

Trung bÃ¬nh (5â€“7)

Cao (quality > 7)

XÃ¢y dá»±ng, huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ hai mÃ´ hÃ¬nh há»c mÃ¡y:

Logistic Regression

Decision Tree (ID3)

Táº¡o giao diá»‡n ngÆ°á»i dÃ¹ng (Tkinter) cho phÃ©p nháº­p thÃ´ng sá»‘ rÆ°á»£u vÃ  dá»± Ä‘oÃ¡n cháº¥t lÆ°á»£ng.

ğŸ“Š Dá»¯ liá»‡u & Ä‘áº·c trÆ°ng

Bá»™ dá»¯ liá»‡u gá»“m 11 thuá»™c tÃ­nh hÃ³a há»c:
fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol.

NhÃ£n Ä‘áº§u ra (quality) Ä‘Æ°á»£c quy Ä‘á»•i thÃ nh 3 lá»›p Ä‘á»ƒ phÃ¹ há»£p bÃ i toÃ¡n phÃ¢n loáº¡i.

âš™ï¸ Quy trÃ¬nh thá»±c hiá»‡n

Tiá»n xá»­ lÃ½ dá»¯ liá»‡u

LÃ m sáº¡ch dá»¯ liá»‡u, xá»­ lÃ½ thiáº¿u.

Chuáº©n hÃ³a Ä‘áº·c trÆ°ng (StandardScaler).

Chia dá»¯ liá»‡u Train/Test (80/20).

KhÃ¡m phÃ¡ dá»¯ liá»‡u (EDA)

Heatmap tÆ°Æ¡ng quan.

PhÃ¢n cá»¥m báº±ng K-Means Ä‘á»ƒ quan sÃ¡t xu hÆ°á»›ng cháº¥t lÆ°á»£ng.

Huáº¥n luyá»‡n mÃ´ hÃ¬nh

Logistic Regression (multinomial) + GridSearch tÃ¬m tham sá»‘ tá»‘i Æ°u.

Decision Tree (ID3 â€“ entropy) + tá»‘i Æ°u siÃªu tham sá»‘.

ÄÃ¡nh giÃ¡

Accuracy, F1-macro.

Confusion Matrix.

XÃ¢y dá»±ng giao diá»‡n Ä‘á»“ há»a (Tkinter)

Cho phÃ©p nháº­p 11 Ä‘áº·c trÆ°ng rÆ°á»£u.

Chá»n mÃ´ hÃ¬nh (Logistic/Decision Tree).

Tráº£ káº¿t quáº£ & Ä‘á»™ tin cáº­y.

Hiá»ƒn thá»‹ hÃ¬nh minh há»a mÃ´ hÃ¬nh (cÃ¢y quyáº¿t Ä‘á»‹nh / trá»ng sá»‘ logistic).

ğŸ“ˆ Káº¿t quáº£
MÃ´ hÃ¬nh	Accuracy	F1-macro	Nháº­n xÃ©t
Decision Tree (ID3)	0.9245	0.37	Hiá»‡u suáº¥t tá»‘t, mÃ´ táº£ Ä‘Æ°á»£c quan há»‡ phi tuyáº¿n, dá»… trá»±c quan hÃ³a
Logistic Regression	0.4612	0.31	Tá»‘c Ä‘á»™ nhanh nhÆ°ng khÃ´ng phÃ¹ há»£p dá»¯ liá»‡u phi tuyáº¿n

â¡ï¸ Decision Tree cho káº¿t quáº£ vÆ°á»£t trá»™i, phÃ¹ há»£p nháº¥t cho bÃ i toÃ¡n nÃ y.

ğŸ’¡ HÆ°á»›ng phÃ¡t triá»ƒn

Káº¿t há»£p mÃ´ hÃ¬nh Random Forest Ä‘á»ƒ giáº£m overfitting.

Thá»­ nghiá»‡m SVM hoáº·c XGBoost.

DÃ¹ng SMOTE Ä‘á»ƒ xá»­ lÃ½ máº¥t cÃ¢n báº±ng lá»›p.

Tá»‘i Æ°u tham sá»‘ sÃ¢u hÆ¡n báº±ng Grid Search má»Ÿ rá»™ng.

ğŸ–¥ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

Python

scikit-learn

pandas, numpy

matplotlib, seaborn

Tkinter (giao diá»‡n ngÆ°á»i dÃ¹ng)
